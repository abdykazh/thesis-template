\documentclass[../main.tex]{subfiles}

\begin{document}

\chapter{Background}

\section{Docker \& Kubernetes} 

A container is a standardized software unit that packages code and its dependencies, ensuring the application runs consistently across different environments. According to the 2024 Stack Overflow Developer Survey \cite{surveystackoverflow}, Docker is the most widely used tool among professional developers, with 59\% adoption. Docker is an open platform for developing, shipping, and running applications, enabling efficient containerization and deployment across various environments. A Docker container image is a lightweight, standalone package that includes everything needed to execute an application, such as code, runtime, system tools, libraries, and settings. At runtime, container images become active containers, enabling reliable and efficient application deployment. \cite{container}

While Docker containers simplify application packaging and deployment, Kubernetes extends this by providing orchestration, automation, and scalability for managing containerized applications across multiple environments. Originally designed by Google, the project was first released in 2015. Kubernetes is an open-source platform designed for managing containerized workloads and services, supporting automation and scalable deployments. It facilitates high availability, failover management, and resource optimization for distributed systems. 

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/3-background/kubernetes/container_evolution.png}
    \caption{Container Evolution. Source: https://kubernetes.io/images/docs/Container\_Evolution.svg (accessed: 18.03.2025)}
    \label{fig:container_evolution}
\end{figure}

Figure \ref{fig:container_evolution} depicts the progression of deployment methods, from traditional physical servers to virtual machines (VMs) and, ultimately, containerized environments. Initially, applications operated directly on physical servers, which often led to challenges in resource allocation and higher operational costs. The introduction of virtualization addressed these issues by enabling multiple VMs to run on a single server, improving resource utilization, scalability, and security. Containers further enhanced deployment efficiency by allowing applications to share the host operating system while maintaining isolation. Due to their lightweight and portable nature, containers have become well-suited for cloud environments. Kubernetes plays a key role in managing these containerized applications by providing a structured framework for deployment and operation. \cite{kubernetes}

\section{Helm}

Helm is a package manager for Kubernetes that simplifies the definition, installation, and management of complex applications using Helm Charts. Users can explore public Helm charts on Artifact Hub. \cite{helm} 

Helm allows users to create, package, and manage Helm charts. It enables storing and retrieving charts from repositories, installing, upgrading, and uninstalling Kubernetes applications, and managing application release cycles. Helm is written in the Go programming language. It uses the Kubernetes client library via REST+JSON and stores data in Kubernetes Secrets without requiring an external database. Configuration files are written in YAML where possible. A \textbf{chart} is a package containing Kubernetes application definitions. A \textbf{config} consists of configuration settings applied to a chart. A \textbf{release} is a deployed instance of a chart with a specific configuration. The \textbf{Helm Client} is a command-line tool that allows users to manage charts, repositories, and releases. The \textbf{Helm Library} executes Helm operations, interacts with the Kubernetes API, and handles installation, upgrades, and uninstallation. \cite{helmarchitecture}

\section{Rancher}

Rancher is a Kubernetes management platform that enables cluster deployment across any infrastructure. It supports multiple provisioning methods, including hosted providers, on-premise setups, and importing existing clusters. 

Beyond deployment, Rancher offers centralized authentication, role-based access control (RBAC), monitoring, alerting, and integration with external logging tools. It also supports Helm-based application deployment and includes Fleet, a built-in GitOps solution for automating workload management. \cite{rancher}

Rancher provides a user-friendly interface that simplifies application workload management. It is designed to be accessible for those with limited Kubernetes expertise.

The figure below illustrates Rancher’s role in IT and DevOps environments. Development teams deploy applications on their chosen public or private cloud platforms, while IT-administrators oversee operations, enforce policies, and maintain visibility across users, clusters, and cloud infrastructure.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{img/3-background/rancher/rancher_devops_teams.png}
    \caption{Rancher’s Role Between DevOps Teams and IT Administrators. 
    Source: https://ranchermanager.docs.rancher.com/assets/images/platform-9c0c4130a7a0828898dbc7af56f76df7.png (accessed: 19.03.2025)}
    \label{fig:rancher_devops_teams}
\end{figure}

Rancher \gls{cli} is available as a tool to interact with Rancher on a local work station along with the \gls{ui}. The Rancher interface provides access to all management and configuration tools available in Rancher. It offers greater functionality than the \gls{cli} and is the only method for installing dashboard applications or Rancher feature charts.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/3-background/rancher/rancher-gui.png}
    \caption{Rancher’s main page of the \texttt{"testcluster"} on the company's premises.}
    \label{fig:rancher_gui}
\end{figure}

Figure \ref{fig:rancher_gui} displays the main page of the RKE1 cluster \texttt{"testcluster"} in a running state. Figure \ref{fig:rancher_gui_apps} highlights the available menu options and the Apps/Charts section, showcasing the Helm charts that can be installed on the cluster with a single click.
%what is RKE1?
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/3-background/rancher/rancher_gui_apps.png}
    \caption{Rancher’s page under Apps/Charts of the \texttt{"testcluster"} on the company's premise.}
    \label{fig:rancher_gui_apps}
\end{figure}

\textbf{Rancher Architecture}

Figure \ref{fig:rancher_users} illustrates the Rancher architecture, assuming Rancher is managing an RKE2 cluster \texttt{"User Cluster 1"} and a K3s cluster \texttt{"User Cluster 2"}.
%what about RKE1 is it also from Kubernetes?
RKE2 is Rancher's enterprise-ready Kubernetes distribution, designed for security, stability, and compliance \cite{rke2}. K3s is a lightweight Kubernetes version, optimized for resource efficiency and edge computing environments \cite{K3s}.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.3]{img/3-background/rancher/rancher_users.png}
    \caption{Rancher Users, Source: https://www.rancher.academy/courses/take/rancher-basics/lessons/42838206-the-rancher-architecture-lesson (accessed 10.12.2024)}
    \label{fig:rancher_users}
\end{figure}

Assume there are two users: IT-administrator Bob and developer Alice. Bob requires access to all clusters, while Alice only needs access to the RKE2 cluster.

For every Kubernetes API call, the Authentication Proxy in the Rancher Server sets the appropriate impersonation headers and forwards the request to the control plane of the respective Kubernetes cluster via a tunnel. This tunnel is managed by the Cluster Agent, which runs on every downstream cluster and connects to the corresponding Cluster Controller residing on the Rancher Management Servers. 

Each Cluster Controller is responsible for:

\begin{enumerate}
    \item Monitoring resource changes in downstream clusters.
    \item Reconciling the cluster state to match the desired configuration.
    \item Configuring access control policies for clusters and projects.
    \item Provisioning clusters by invoking the necessary machine drivers and Kubernetes engines.
\end{enumerate}

If the Cluster Agent becomes unavailable in a cluster, the Node Agent — deployed as a DaemonSet on every worker node—takes over tunneling responsibilities.  

The Node Agent is typically responsible for:

\begin{enumerate}
    \item Performing cluster-wide operations, such as Kubernetes version upgrades.
    \item Creating and restoring RKE snapshots. 
\end{enumerate}
% Explain that this is what you are doing for your thesis and that this explains the direct arrow Alice -> RKE2
Alice and Bob can access the RKE2 cluster via the Rancher \gls{ui} or \gls{cli} using Authorized Cluster Endpoint to perform necessary operations.  

When Rancher provisions Kubernetes using RKE/RKE2, an Authorized Cluster Endpoint can be configured. This enables a direct connection to the Kubernetes API of the downstream cluster, bypassing the Authentication Proxy. When enabled, Rancher generates additional Kubernetes API endpoints in the kubeconfig file, allowing direct cluster access. This file includes credentials for both kubectl and Helm. The Authorized Cluster Endpoint is not available for imported clusters or clusters in hosted Kubernetes providers (e.g., Amazon EKS).
The Authorized Cluster Endpoint ensures continuous access to Rancher-managed clusters, e.g. during planned downtime of the Rancher Management Server or during an unplanned outage. Once Rancher is restored, changes made during downtime will be synchronized through the existing tunneling mechanism.

Every user is authenticated against the Rancher Authentication Proxy, based on their assigned global permissions and roles. Permissions determine who can access a given resource. Roles define what actions a user can perform on those resources. This model ensures fine-grained access control across multiple clusters. \cite{rancherbasics}
\section{Graylog Open}
%maybe just call this section Graylog?
Graylog is a centralized log management solution designed for log aggregation, analysis, and management. It was developed to address usability and cost challenges associated with existing log management platforms. It started in 2009 as an open-source project.

Graylog is an integrated tool where log messages are stored with Elasticsearch/OpenSearch, while associated metadata is stored in MongoDB. An application process serves as a central log server; a web interface can be used for querying and visualizing the data and for managing Graylog. A major advantage of Graylog is the ability to restrict access to the system. Graylog allows users to search, filter, and categorize log messages using configurable rules, and to set up dashboards. \cite{hopf2016elasticsearch}

Graylog Open is the free, open-source version of Graylog, providing core log management capabilities, including log data collection, enrichment, storage, and analysis. The development of Graylog Open is supported by community contributions. Graylog Open, licensed under \gls{sspl}, is a self-managed solution widely used by organizations and individuals for processing and analyzing log data. 

In addition to the open-source version, enterprise options are available for organizations requiring advanced security, compliance, and scalability features.

The Graylog Stack consists of:  

\begin{enumerate}
    \item{Graylog} - Responsible for log processing and visualization 
    \item{Data Node} – Handles storage and indexing
    \item{MongoDB} – Stores metadata and configuration
\end{enumerate} 

A Graylog Data Node manages ElasticSearch/OpenSearch, which provides indexing and search functionality.  

A conventional deployment is the recommended architecture for large-scale production environments, particularly those handling high log volumes or supporting multiple Graylog users. This setup distributes Graylog components across multiple nodes, ensuring high availability, scalability, and fault tolerance.  

The accompanying diagram \ref{fig:graylog_deployment} provides a visual representation of this architectural model, illustrating its components, data flow, resource allocation, hardware, and storage requirements.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.35]{img/3-background/graylog/deployment.png}
    \caption{Graylog Conventional Deployment. Source: https://go2docs.graylog.org/current/resources/images/graylog\_architecture/conventionaldeploymentarchitecture.png (accessed 10.03.2025)}
    \label{fig:graylog_deployment}
\end{figure}

\section{Kubernetes Logging Architecture}

\cite{loggingkubernetes}

\section{Centralized Logging Solution}

\subsection{Architecture}
%which centralized logging solution? Yours?
The centralized logging solution comprises two applications:

\begin{enumerate}
    \item Logging Operator – Forwards logs from the Kubernetes cluster. 
    \item Graylog – Serves as the logging solution for storing, processing, and analyzing logs.
\end{enumerate}

\subsection{Logging Operator}

The Logging Operator \cite{logoperator} automates the deployment and configuration of a Kubernetes logging pipeline, simplifying log management. It deploys a Fluent Bit DaemonSet 
%what does it do? Whats so special about it? 
on each node to collect container and application logs, enriches them with Kubernetes metadata, and forwards them to a log forwarder. The operator manages both log collectors and forwarders, as well as routing rules for log distribution. Supported log forwarders include **Fluentd** and **syslog-ng** (via AxoSyslog).
%whats up with **?
Kubernetes logs are generated by various components, including the Kubernetes API server, Kubelet, container runtime, and applications running within the cluster. The Logging Operator collects and aggregates these logs, then forwards them to Graylog for centralized storage and analysis.

\begin{figure}[H]
        \centering
        \includegraphics[]{img/3-background/centralized_logging/architecture.png}
        \caption{Centralized Logging Solution Architecture. Source: https://kube-logging.dev/docs/img/logging\_operator\_flow.png (adjusted) (accessed: 10.12.2024)}
        \label{fig:centralized_logging_architecture}
\end{figure}

\subsubsection{Cluster Logging Configuration}
%weird sentence. What is Fluent Bit? Why "use" twice in the phrase?
Logging operator currently uses Fluent Bit as log collector agents, which will be used.

\subsubsection{Flow Resources}

There are four main resources in the logging setup for Fluentd: \textbf{Flow, Output, ClusterFlow, and ClusterOutput} \cite{logconfig}.  
%maybe explain really quick what Fluentd or in general a log forwarder is
\begin{itemize}
    \item \textbf{Flow:} Directs logs from applications within a specific namespace to designated outputs.
    \item \textbf{Output:} Defines a destination where Fluentd Flows can send log messages. Since it is a namespaced resource, only Flows within the same namespace can access it.
    \item \textbf{ClusterOutput:} Similar to Output but without namespace restrictions, allowing logs from multiple namespaces to be forwarded to a central destination.
    \item \textbf{ClusterFlow:} Functions like Flow but operates across all namespaces, enabling cluster-wide log forwarding.
\end{itemize}

The Logging Operator can be installed on Kubernetes environments using Helm charts.
%this format is very messy here
Level

https://logging.paluch.biz/syslog-level-mapping.html

https://docs.rs/gelf/latest/gelf/enum.Level.html

GELF / Syslog
Emergency (0)
Alert (1)
Critical (2)
Error (3)
Warning (4)
Notice (5)
Informational (6)
Debug (7)

Level field wie wird überschrieben?

\subsubsection{Flow Configuration}

Flow defines a logging flow for Fluentd with filters and outputs.

The Flow is a namespaced resource, so only logs from the same namespaces are collected. You can specify match statements to select or exclude logs according to Kubernetes labels, container and host names. Match statements are evaluated in order they are defined and processed only until the first matching select or exclude rule applies. 

A Flow consists of filters and outputs and only collects logs from the same namespace. Match statements allow filtering logs based on Kubernetes labels, container names, or hostnames. 

\subsubsection{Filtering and Processing Logs}

Additionaly, you can define one or more filters within a Flow \cite{fluentdfilters}. Filters can perform various actions on the logs. A Flow can contain multiple filters, applied in sequence. These filters enable:

\begin{itemize}
    \item \textbf{Log enrichment} with additional data
    \item \textbf{Modification of log content}
    \item \textbf{Extraction of specific values} from log entries
\end{itemize}


\subsection{Rancher Logging App}

The Logging operator has also been integrated as Rancher's logging solution, replacing the previous in-house system. To enable logging for a Rancher-managed cluster, the "Logging" App from the Apps page in the Rancher \gls{ui} can be installed.

\begin{figure}[H]
        \centering
        \includegraphics[]{img/3-background/centralized_logging/logging_app.png}
        \caption{Rancher Logging App.}
        \label{fig:logging_app}
\end{figure}

The key advantage of this deployment is the integrated Role-Based Access Control (RBAC). Rancher "Logging" App provides two roles: \texttt{logging-admin} and \texttt{logging-view}.
% do I understand correctly that those two roles can also be called Project Owners and Project Members? Maybe explain that

\begin{figure}[H]
        \centering
        \includegraphics[scale=0.7]{img/3-background/rancher_logging_app/roles.png}
        \caption{Roles for Rancher "Logging". Source: https://ranchermanager.docs.rancher.com/integrations-in-rancher/logging/rbac-for-logging (accessed: 19.03.2025)}
        \label{fig:roles}
\end{figure}

\clearpage
\section{Kano Model}

Kano’s Model of Attractive Quality \cite{kano1984attractive} introduced in 1984, has become one of the most widely used quality models among researchers across various industries. 

\begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{img/3-background/kano/kano.png}
        \caption{Kano’s model of customer satisfaction \cite{kanomodel1996}.}
        \label{fig:kano}
\end{figure}

The horizontal axis of the Kano diagram in Figure \ref{fig:kano} indicates how fully functional some aspect of a product is, and the vertical axis 
indicates how satisfied the customer is. Indifference would be plotted on \ref{fig:kano} roughly along the horizontal axis, meaning that the customer is neither satisfied nor dissatisfied. The line going through the origin at 45 degrees graphs the situation in which customer satisfaction is simply proportional to whether requirement is full filled. They are defined as “One-dimensional” customer requirements. Curves labeled as "Must-be" and 
"Attractive" requirements. The Must-be curve indicates aspects where the customer is more dissatisfied when the product is less functional, but where 
the customer’s satisfaction never rises above neutral no matter how functional the product becomes. The Attractive curve indicates areas in which 
the customer is more satisfied when the product is more functional but is not dissatisfied when the product is less functional.  \cite{berger1993kano}

The Kano model uses a structured questionnaire to classify such One-dimensional, Attractive, Must-be, and Indifferent customer requirements quality attributes of a product or service. Each attribute is assessed through two paired questions: the \textbf{functional question}, which asks about the consumer’s reaction when the attribute is present, and the \textbf{dysfunctional question}, which asks their response when it is absent. The collected data is analyzed using a specialized evaluation table to categorize attributes for each respondent. The final classification is derived by examining the frequency of individual responses. \cite{kanomodel1996}

\begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{img/3-background/kano/kanomodel.png}
        \caption{The Kano Method \cite{kanomodel1996}.}
        \label{fig:kanomodel}
\end{figure}

The Figure \ref{fig:kanomodel} shows the questionnaire and further analysis workflow. Based on the responses to the two parts of the question in figure, the product feature can be classified into one of six categories: 
\begin{enumerate}
    \item[] A=Attractive 
    \item[] M=Must-be 
    \item[] O=One-dimensional 
    \item[] I=Indifferent 
    \item[] R=Reversel 
    \item[] Q=Questionable 
\end{enumerate}

The other two categories indicate the following situations: There is a contradiction in the customer’s answers to the questions (=Questionable); or our apriori judgment of functional and dysfunctional was the reverse what the customer feels (=Reverse). \cite{berger1993kano}

\textbf{Analyzing the Results}

\textbf{Simple method}

The easiest method is evaluation and interpretation according to the frequency of answers. A simple way to rank the requirements is to score each according to the mode (the most frequently occurring dimension) in each row of the tabulation matrix. One can also look at the second most frequent dimension for each requirement. \cite{berger1993kano}

\textbf{Continuous scale}

 Each response is first converted onto a numerical scale, with more weight attached to positive end of the scale.  The logic for the asymmetrical scale 
(beginning from -2, rather than -4) is that Must-be and One-dimensional are stronger responses than Reverse or Questionable. The Reverse-type responses are given less weight by being pulled toward 0. 

\begin{table}[H]
    \centering
    \begin{tabular}{lccccc}
        \hline
        & Dislike & Tolerate & Neutral & Expect & Like \\
        \hline
        Feature is present & -2 & -1 & 0 & 2 & 4 \\
        Feature is absent  & 4  & 2  & 0 & -1 & -2 \\
        \hline
    \end{tabular}
    \caption{Kano Model Evaluation Table}
    \label{tab:kano}
\end{table}

Next, all functional and dysfunctional responses are averaged together which results in a pair of numbers (one for functional, one for dysfunctional). This pair represents the aggregate total of all the different opinons of the participants.

The pairs for all features are plotted onto a grid, with the different areas of the grid mapped out to Kano categories. The averages should mostly fall in the range 0 to 4, since negative values are either Questionables or Reverses.

\begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{img/3-background/kano/analysis.png}
        \caption{ \cite{berger1993kano}.}
        \label{fig:analysis}
\end{figure}

\textbf{Customer satisfaction coefficient (CS coefficient)}

\cite{timko1993}

\begin{equation}
    \text{Better} = \frac{A + O}{A + O + M + I}
\end{equation}

\begin{equation}
    \text{Worse} = - \frac{O + M }{A + O + M + I}
\end{equation}

total = (attractive (A) + one-dimensional (O) + must\_have (M) + indifferent (I))

better = ((attractive (A) + one-dimensional (O)) / total

worse = ((one-dimensional (O) + must\_have (M)) / total) * -1

\begin{figure}[H]
        \centering
        \includegraphics[]{img/3-background/kano/2dkano.png}
        \caption{ \cite{berger1993kano}.}
        \label{fig:2dkano}
\end{figure}

\section{Firm's Infrastructure}

The company's resources will be utilized for evaluation. A separate test Kubernetes cluster within the company's infrastructure was used for evaluation, followed by integration of the solution into productive cluster.
The logs will be analyzed with the company's consent.

Two RKE1 clusters are deployed on the company's infrastructure were used:

\begin{enumerate}
    \item[] \texttt{testcluster} 
    
    A test cluster: Kubernetes v1.28.10 Amd64, 12 cores, 47 GiB, 3 Nodes
    \item[] \texttt{devops} 
    
    A productive cluster: Kubernetes v1.26.15 Amd64, 72 cores, 533Gib, 18 Nodes
\end{enumerate}

Since 2020, the deployment of internal applications has transitioned to Helm Charts \cite{helm} wherever possible. Helm Charts enable the pre-configured provisioning of all components, allowing applications to be installed in Kubernetes with a single click. Once created, the Helm Chart is be uploaded to the company's local Git repository, which Rancher checks every five minutes to update the application directory accordingly.

\section{Management of Application Logs at the Firm}

Since the firm consists of many different teams, each with its own customers and software needs, the current situation regarding log management will be explained by use cases from the internal Kubernetes cluster.  

The following use cases illustrate their current state of logging, future expectations, and requirements for the logging solution. These observations are made by interviewing the individuals responsible for the respective applications. 

Four use cases will be presented: GitLab, SKPBO, Rezert, and CLD \& METAPlus. These use cases represent three internally developed applications and one third-party application (GitLab).

\subsection{Use Case: Gitlab}

GitLab is a web-based platform for version control and collaboration on software projects. It is based on Git and offers features such as repository management, Continuous Integration (CI), Continuous Delivery (CD), code review, and issue tracking. \cite{gitlab}

A GitLab instance is used by the firm internally, running on a productive Kubernetes cluster. In 2022, the central internal GitLab repository was migrated to the Kubernetes infrastructure. There is a test and a productive instance running on the cluster. For internal productive applications, there is a functional application manager and a technical application manager. An employee interviewed is registered as the technical application manager for GitLab.

\subsubsection{Current State}

The internal GitLab instance is running productively on the Kubernetes cluster. There are two namespaces: one for the test instance and one for the productive GitLab instance. 

Updates are first carried out in the test namespace. If successful, the productive instance is then updated. Various unusual errors have occurred in GitLab in the past, the origins of which were unclear. To further illustrate the current state, a scenario describing an issue in a production GitLab application is presented. \\

\textit{Scenario: Error in a production GitLab Application}

\begin{enumerate}
    \item Reporting the Issue: 
    
    An employee noticed a software malfunction and reported it to the technical application manager.
    
    \item Inspection of GitLab Components: 
    
    The manager inspects the software, verifies the malfunction, and proceeds to examine running pods individually (Figure \ref{fig:gitlab}).
    
    \begin{figure}[H]
        \centering
        \includegraphics[]{img/3-background/gitlab/gitlab.png}
        \caption{Gitlab pods view on Rancher.}
        \label{fig:gitlab}
    \end{figure}
    
    \item Log Analysis: 
    
    The responsible person begins by searching for errors using the Rancher \gls{ui}. However, the search function in Rancher often malfunctions. Alternatively, logs can be downloaded manually from the Rancher \gls{ui} (\texttt{"Download"} button) or retrieved using the \texttt{"kubectl logs"} command. The downloaded logs only include data starting from the container’s initial launch. However, this approach comes with the drawback of discarding potentially valuable historical data that might be useful for diagnosing long-term or recurring issues. Since there is no backup mechanism currently in place, logs are lost after container restarts, and no history is saved.

    \begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{img/3-background/gitlab/rancher_logs_gui.png}
        \caption{Gitlab Rancher GUI logs view.}
        \label{fig:rancher_logs_gui}
    \end{figure}

    \item Findings: 
    
    The person responsible was often unable to find any relevant log entries or errors, as the available logs either lacked useful information (such as irrelevant \texttt{"Timeout"} or \texttt{"Merge conflict"} errors). As a result, the workflow did not contribute to identifying or resolving the issue. Due to the overwhelming volume of logs and the need to inspect each pod’s logs separately, it was likely that critical errors went unnoticed, as there was no way to view the entire timeline in a consolidated manner.

    \item Resolution
    
    The only effective solution in such cases is to redeploy the application.
    
\end{enumerate}

At present, effective log analysis is not feasible. The sheer volume of data makes manual analysis impractical, while automated methods (e.g., using grep or writing Python script) demand significant effort to identify relevant information, for which there is neither the time nor the inclination. Consequently, the current solution is to redeploy the whole application, with the hope that the issue will be resolved through the restart process.

\subsubsection{Challenges \& Requirements}

\begin{enumerate}
    \item[-] GitLab test and production instances have approximately 20 pods running. Merging logs from different pods in chronological order is necessary.
    
    \item[-] Identify and eliminate recurring issues, which are unrelated to GitLab but still impact its functionality (e.g., a recurring request to an unavailable service).
    
    \item[-] Possibility of quick search: GitLab generates a large number of log messages. It should be possible to quickly search for specific entries, such as error logs.
    
    \item[-] Searching for specific values: e.g., filtering by a specific user or project. Extracting values as separate attributes to identify all logs relevant to the user or project that reported an error.
    
    \item[-] Alerts for critical errors: GitLab logs many errors that are not relevant to administrators (e.g., a Git merge request failing due to a merge conflict). Such errors should be ignored.
    
    \item[-] \gls{ad} user management is desirable to avoid manually creating user accounts for application managers.
\end{enumerate}

\subsection{Use Case: SKPBO}

The company's internal Kubernetes cluster serves as a test environment for development and test of SKPBO applications. It is not yet running productively in a containerized manner due to regulatory and technical reasons; however, the transition to a containerized production environment is in planning. The internal Kubernetes cluster runs in our own data center, providing greater flexibility and reducing management and administrative overhead. However, the productive instance runs on the infrastructure of a sister company, which is regulated by a contract with the customer. Since the provisioning and management of the production infrastructure are handled entirely by a sister company, the transition to containerized technologies often takes a long time.

SKPBO is a project that includes 5 different business applications accessible via a web browser. All requests pass through the Keycloak Identity Provider, which integrates multiple authentication mechanisms, permissions, and role management from other internal systems.

Keycloak is an open-source solution that provides a unified interface, abstracting and standardizing access to various business applications. The primary users are employees of a large bank. After authentication, they are directed to the business applications, which are developed internally using Java Spring Boot.

\subsubsection{Current State}

\textbf{Development/Test Environment (Kubernetes Cluster)}

The Kubernetes cluster is managed internally running on company's data center. Keycloak and Spring Boot applications run in their respective namespaces and communicate with each other. No centralized logging is in place. The internal Kubernetes environment is used for development and testing to prepare for a future transition to the production environment.

\textbf{Production Environment (Linux Servers)}

The production infrastructure is managed by a sister company.Applications run in production on Linux servers, where Spring Boot applications are manually started and managed. No centralized logging in place, logs are stored directly on the servers and only reviewed manually in case of incidents. The sister company offers Splunk as a centralized logging solution, but the team has not yet requested it.

\subsubsection{Challenges \& Requirements}

\begin{enumerate}
    \item[-] Aggregate logs from multiple instances centrally. Previously, there was only one Keycloak instance, so manual log searches were manageable. Multiple instances of Keycloak are planned to be used in the future, making manual log analysis significantly more complex.

    \item[-] Targeted Search \& Alerting
    
    Filter logs based on specific criteria (e.g., level=error).
    
    Set up automatic alerts for critical log entries.

    \item[-] OpenID Connect (OIDC) for Authentication

    Integration with Keycloak to allow external administrators to access logs without manually creating user accounts.

    \item[-] Efficient Storage Management
    
    Since the infrastructure is managed by the sister company, storage space is limited. Currently, logging is kept to a minimum to prevent the file system from filling up. Proposed idea: Send logs to Graylog and automatically delete them from servers to save storage space.

    \item[-] Access Control \& Data Protection

    A centralized access control model is needed. A Graylog administrator should be assigned to manage log streams in production. Logs contain sensitive/private information, so access rights must be clearly defined.

    \item[-] Expansion into a Comprehensive Monitoring Solution

    Extend beyond logging to capture additional system metrics.
    
    \item[-] Log Filtering \& Downloading
    
    Enable filtering and downloading of logs to facilitate easy sharing when needed.

    \item[-] Installation

    A possibility to deploy Graylog not only on a cluster but also directly on a server if the team decides to use it before transitioning to Kubernetes.

\end{enumerate}

\subsection{Use Case: CLD \& METAPlus}

Credit Loss Database (CLD)

The CLD is the central system for processing, calculating, and storing risk provisions for the bank's defaulted transactions.

METAPlus

The tool is used in the creditworthiness-driven savings bank lending business for new transactions. It is used for covenant monitoring and enables the processing of credit monitoring templates as well as other templates within a fully electronic workflow.
The primary users are employees of a large bank.

\subsubsection{Current State}

\textbf{Development/Test Environment (Kubernetes Cluster)}

The Kubernetes cluster is managed internally, running on company's data center. In the Kubernetes cluster every GitLab push automatically triggers a CLD/METAPlus release, including WildFly and a database server. No centralized logging is in place. The internal Kubernetes environment is used for development and testing to prepare for a future transition to the production environment.

\textbf{Production Environment}

Applications run productively at the customer’s site. The customer fully manages operations. However, in the case of major incidents where the bank cannot resolve the issue, the customer reaches out to the team and sends logs via E-mail.

\subsubsection{Challenges \& Requirements}

Since the instances on the Kubernetes cluster are only used to simplify the development process by automatically building and deploying branches for testing, the team lead does not see the need for a centralized logging solution. As they do not manage production, he believes that checking logs directly in the IDE during development is sufficient and that using a logging solution would not bring significant benefits.

Historically, when containers weren’t updated for extended periods, log files within them could grow significantly, leading to excessive memory usage in the pod. To mitigate this, log-rolling mechanisms (e.g., via libraries like Log4j) were implemented. These mechanisms automatically delete logs after they reach a certain size or age.

For in-house developed applications, it is possible to view logs through the Kubernetes plugin in IntelliJ or other IDEs. This eliminates the need to manually retrieve logs via the Rancher GUI or command line. 

Option 1: Via the Rancher interface:

\begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{img/3-background/gitlab/rancher_logs_gui.png}
        \caption{}
        \label{fig:rancher_logs_gui}
    \end{figure}

Option 2: Via the Kubernetes plugin in IntelliJ

\begin{figure}[H]
        \centering
        \includegraphics[scale=0.8]{img/3-background/gitlab/logs_plugin.png}
        \caption{}
        \label{fig:logs_plugin}
    \end{figure}

\subsection{UseCase: Rezert Tool}

An application „Rezert Tool“ was developed for internal use that significantly simplifies the recertification process by automatically processing data from the respective systems into tailored tickets containing all necessary information. As a result, 90\% of recertifications can be completed simply by closing the ticket.
The DevOps Rezert Tool is implemented using microservices based on Kubernetes and Kafka.

\subsubsection{Current State}

The development instance is running productively on one cluster, and the productive instance on the other cluster.

\subsubsection{Challenges \& Requirements}

\begin{enumerate}
    \item[-] Since one process goes through all microservices, we need logs from all of them to trace the entire workflow. Logs need to be combined from different microservices in chronological order.
    \item[-] Quickly access logs in case of incidents.
    \item[-] Search by process: find the process ID field and display logs from different microservices. It is neccesary to identify in which service did an error occur.
    \item[-] Alerts for errors.
    \item[-] Dashboard request: display the entire process flow by process ID, showing logs from each service and whether it was successfully completed (e.g., from request to ticket creation = SUCCESS)
\end{enumerate}



\end{document}
